{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SCIENTIST\n",
    "In this tutorial, I only explain you what you need to be a data scientist neither more nor less.\n",
    "\n",
    "Data scientist need to have these skills:\n",
    "\n",
    "- Basic Tools: Like python, R or SQL. You do not need to know everything. What you only need is to learn how to use python\n",
    "- Basic Statistics: Like mean, median or standart deviation. If you know basic statistics, you can use python easily.\n",
    "- Data Munging: Working with messy and difficult data. Like a inconsistent date and string formatting. As you guess, python helps us.\n",
    "- Data Visualization: Title is actually explanatory. We will visualize the data with python like matplot and seaborn libraries.\n",
    "- Machine Learning: You do not need to understand math behind the machine learning technique. You only need is understanding basics of machine learning and learning how to implement it while using python.\n",
    "\n",
    "As a summary we will learn python to be data scientist !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('column_2C_weka.csv')\n",
    "print(plt.style.available)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. SUPERVISED LEARNING\n",
    "- Supervised learning: It uses data that has labels. Example, there are orthopedic patients data that have labels normal and abnormal.\n",
    "    - There are features(predictor variable) and target variable. Features are like pelvic radius or sacral slope(If you have no idea what these are like me, you can look images in google like what I did :) )Target variables are labels normal and abnormal\n",
    "    - Aim is that as given features(input) predict whether target variable(output) is normal or abnormal\n",
    "    - Classification: target variable consists of categories like normal or abnormal\n",
    "    - Regression: target variable is continious like stock market\n",
    "    - If these explanations are not enough for you, just google them. However, be careful about terminology: features = predictor variable = independent variable = columns = inputs. target variable = responce variable = class = dependent variable = output = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS (EDA)\n",
    "     - In order to make something in data, as you know you need to explore data. Detailed exploratory data analysis is in my Data Science Tutorial for Beginners\n",
    "     - I always start with head() to see features that are pelvic_incidence, pelvic_tilt numeric, lumbar_lordosis_angle, sacral_slope, pelvic_radius and degree_spondylolisthesis and target variable that is class\n",
    "    - head(): default value of it shows first 5 rows(samples). If you want to see for example 100 rows just write head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well know question is is there any NaN value and length of this data so lets look at info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see:\n",
    "\n",
    "- length: 310 (range index)\n",
    "- Features are float\n",
    "- Target variables are object that is like string\n",
    "- Okey we have some ideas about data but lets look go inside data deeper\n",
    "    - describe(): I explain it in previous tutorial so there is a Quiz\n",
    "         - Why we need to see statistics like mean, std, max or min? I hate from quizzes :) so answer: In order to visualize data, values should be closer each other. As you can see values looks like closer. At least there is no incompatible values like mean of one feature is 0.1 and other is 1000. Also there are another reasons that I will mention next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.plotting.scatter_matrix:\n",
    "\n",
    "- green: normal and red: abnormal\n",
    "- c: color\n",
    "- figsize: figure size\n",
    "- diagonal: histohram of each features\n",
    "- alpha: opacity\n",
    "- s: size of marker\n",
    "- marker: marker type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\n",
    "pd.plotting.scatter_matrix(data.loc[:,data.columns != 'class'],\n",
    "                          c=color_list,\n",
    "                          figsize= [15,15],\n",
    "                          diagonal = 'hist',\n",
    "                          alpha=0.5,\n",
    "                          s=200,\n",
    "                          marker = '.',\n",
    "                          edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, as you understand in scatter matrix there are relations between each feature but how many normal(green) and abnormal(red) classes are there.\n",
    "\n",
    "- Searborn library has countplot() that counts number of classes\n",
    "- Also you can print it with value_counts() method\n",
    "\n",
    "This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n",
    "Now lets learn first classification method KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='class', data=data)\n",
    "data.loc[:,'class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NEAREST NEIGHBORS (KNN)\n",
    " - KNN: Look at the K closest labeled data points\n",
    " - Classification method.\n",
    " - First we need to train our data. Train = fit\n",
    " - fit(): fits the data, train the data.\n",
    " - predict(): predicts the data\n",
    "    If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n",
    "Lets learn how to implement it with sklearn\n",
    " - x: features\n",
    " - y: target variables(normal, abnormal)\n",
    " - n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n",
    "knn.fit(x,y)\n",
    "prediction = knn.predict(x)\n",
    "print('Prediction : {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Well, we fit the data and predict it with KNN.\n",
    " - So, do we predict correct or what is our accuracy or the accuracy is best metric to evaluate our result? Lets give answer of this questions\n",
    "Measuring model performance:\n",
    " - Accuracy which is fraction of correct predictions is commonly used metric. We will use it know but there is another problem\n",
    "\n",
    "As you see I train data with x (features) and again predict the x(features). Yes you are reading right but yes you are right again it is absurd :)\n",
    "\n",
    "\n",
    "Therefore we need to split our data train and test sets.\n",
    "\n",
    " - train: use train set by fitting\n",
    " - test: make prediction on test set.\n",
    " - With train and test sets, fitted data and tested data are completely different\n",
    " - train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "    - x: features\n",
    "    - y: target variables (normal,abnormal)\n",
    "    - test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n",
    "    - random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n",
    "- fit(x_train,y_train): fit on train sets\n",
    "- score(x_test,y_test)): predict and give accuracy on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state =1)\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n",
    "knn.fit(x_train,y_train)\n",
    "prediction = knn.predict(x_test)\n",
    "#print('Prediction: {}'.format(prediction))\n",
    "print('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is 86% so is it good ? I do not know actually, we will see at the end of tutorial.\n",
    "Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n",
    "\n",
    "\n",
    "Model complexity:\n",
    "\n",
    " - K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace.\n",
    " - Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n",
    " - If k is big, model that is less complex model can lead to underfit.\n",
    " - At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neig = np.arange(1,25)\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "#loop over different values of k\n",
    "for i,k in enumerate(neig):\n",
    "    #k from 1 to 25(exclude)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Fit with knn\n",
    "    knn.fit(x_train,y_train)\n",
    "    #train accuracy\n",
    "    train_accuracy.append(knn.score(x_train,y_train))\n",
    "    # test accuracy\n",
    "    test_accuracy.append(knn.score(x_test,y_test))\n",
    "\n",
    "    # Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(neig, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neig, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('-value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "print(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point what you learn:\n",
    " - Supervised learning\n",
    " - Exploratory data analysis\n",
    " - KNN\n",
    "     - How to split data\n",
    "     - How to fit, predict data\n",
    "     - How to measure medel performance (accuracy)\n",
    "     - How to choose hyperparameter (K)\n",
    "\n",
    "What happens if I chance the title KNN and make it some other classification technique like Random Forest?\n",
    "\n",
    "- The answer is nothing. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be RandomForestClassifier ) are same. You need to split, fit, predict your data and measue performance and choose hyperparameter of random forest(like max_depth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "- Supervised learning\n",
    "- We will learn linear and logistic regressions\n",
    "- This orthopedic patients data is not proper for regression so I only use two features that are sacral_slope and pelvic_incidence of abnormal\n",
    "    - I consider feature is pelvic_incidence and target is sacral_slope\n",
    "    - Lets look at scatter plot so as to understand it better\n",
    "    - reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\n",
    "data1 = data[data['class'] =='Abnormal']\n",
    "x = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\n",
    "y = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n",
    "# Scatter\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.scatter(x=x,y=y)\n",
    "plt.xlabel('pelvic_incidence')\n",
    "plt.ylabel('sacral_slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "# Predict space\n",
    "predict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n",
    "# Fit\n",
    "reg.fit(x,y)\n",
    "# Predict\n",
    "predicted = reg.predict(predict_space)\n",
    "# R^2 \n",
    "print('R^2 score: ',reg.score(x, y))\n",
    "# Plot regression line and scatter\n",
    "plt.plot(predict_space, predicted, color='black', linewidth=3)\n",
    "plt.scatter(x=x,y=y)\n",
    "plt.xlabel('pelvic_incidence')\n",
    "plt.ylabel('sacral_slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
